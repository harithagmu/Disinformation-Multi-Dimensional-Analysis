# Disinformation-Multi-Dimensional-Analysis
Identifying disinformation has been a challenge although many methods have been proposed recently by identifying emerging trends in the spread of disinformation. The disinformation has been growing rapidly due to the heavy use of social media by people and high level of social engagement. Social media owners are finding ways to tackle the spread of disinformation online due to its spread and global impact. Existing research has been carried out on two broad categories of disinformation that are opinion-based which include fake reviews and fact-based which include fake news. Integrating the socio-political dimension in the analyses of fake news to better understand the nature of information and its distribution helps in automatic detection of the fake news. 
The scope of this project is to develop methods and to experiment with existing methods to identify fake news. These days there are few open source browser extensions for real time detection of misinformation. But the performance of such automatic detection engines has not proven to be reliable due to the real challenge in defining what fake news is and the limited availability of annotated data with gold standard labels. Human labeling  also has an impact/bias of the misinformation because they filter the information they read (For example, people read/watch what they like and ignore what they don’t).
Given the annotated data from several sources, the challenge is to use the right metric that could evaluate the performance of deep learning models. This is possible by using a combination of metrics. A combination of human input to state-of-the-art deep learning methods could reap significant improvements in detecting misinformation and generalizability of the results.

The following annotated datasets were identified for the analysis. Each of the datasets come from different sources such as twitter,Politifact and BSDetector. Each of the datasets are descibed below:

LIAR dataset: This is publicly available and used for the task of fake news detection. This dataset was collected over a decade and contains 12,836 human-labeled short statements, which are sampled from various contexts from POLITIFACT.COM, which provides detailed analysis report and links to source documents for each case. The labels for news truthfulness are fine-grained classes: pants-fire, false, barely-true, half-true, mostly true, and true. 

FakeNewsNet: This contains two datasets collected using ground truths from Politifact and Gossipcop. The output labels include ‘real’ and ‘fake’. PolitiFact is a website that provides fact-checking evaluation for news articles that were annotated as fake or real by journalists and domain experts. GossipCop is a website for fact-checking entertainment stories collected from different media platforms. It rates a news story on the scale of 0 to 10 to classify the degree of fakeness.
